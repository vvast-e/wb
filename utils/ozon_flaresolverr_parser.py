#!/usr/bin/env python3
"""
üöÄ OZON FLARESOLVERR –ü–ê–†–°–ï–† - –°–¢–ê–ë–ò–õ–¨–ù–´–ô –û–ë–•–û–î –ê–ù–¢–ò–ë–û–¢–ê üöÄ

–°–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ Ozon —á–µ—Ä–µ–∑ FlareSolverr.
–í–∫–ª—é—á–∞–µ—Ç:
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å FlareSolverr –¥–ª—è –æ–±—Ö–æ–¥–∞ 403 –∞–Ω—Ç–∏–±–æ—Ç–∞
- –ü–∞—Ä—Å–∏–Ω–≥ —Ü–µ–Ω –∏ –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ HTML
- –ü–æ–¥–¥–µ—Ä–∂–∫—É —Ä–µ–∑–∏–¥–µ–Ω—Ç–Ω—ã—Ö –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏
- –°—Ç–∞–±–∏–ª—å–Ω—É—é —Ä–∞–±–æ—Ç—É —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
"""

import asyncio
import json
import os
import random
import re
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import quote, urljoin, urlparse
import uuid

try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False
    print("‚ùå requests –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install requests")

try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False
    print("‚ùå BeautifulSoup –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install beautifulsoup4")

class OzonFlareSolverrParser:
    """–°—Ç–∞–±–∏–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä Ozon —á–µ—Ä–µ–∑ FlareSolverr"""
    
    def __init__(self, proxy_config: Optional[Dict] = None, flaresolverr_url: str = "http://localhost:8191/v1"):
        """
        proxy_config = {
            'scheme': 'http',
            'host': 'your.proxy.host',
            'port': 8080,
            'username': 'user',
            'password': 'pass'
        }
        """
        self.proxy_config = proxy_config
        self.flaresolverr_url = flaresolverr_url
        self.session = None
        self.cookies = {}
        self.device_id = str(uuid.uuid4())
        self.session_id = str(uuid.uuid4())
        
        # –ö—ç—à –¥–ª—è HTML —Å—Ç—Ä–∞–Ω–∏—Ü
        self.html_cache = {}
        self.cache_timeout = 300  # 5 –º–∏–Ω—É—Ç
        
        # –ú–æ–±–∏–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è FlareSolverr
        self.mobile_headers = {
            'User-Agent': 'Mozilla/5.0 (Linux; Android 12; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.210 Mobile Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-User': '?1',
            'Sec-Fetch-Dest': 'document',
            'Cache-Control': 'max-age=0',
        }
        
    def _build_proxy_url(self) -> Optional[str]:
        """–°–æ–±–∏—Ä–∞–µ—Ç URL –ø—Ä–æ–∫—Å–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞"""
        if not self.proxy_config:
            return None
        
        config = self.proxy_config
        auth = ""
        if config.get('username') and config.get('password'):
            auth = f"{config['username']}:{config['password']}@"
        
        return f"{config.get('scheme', 'http')}://{auth}{config['host']}:{config['port']}"
    
    async def __aenter__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
        if not HAS_REQUESTS:
            raise ImportError("requests –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞!")
        
        if not HAS_BS4:
            raise ImportError("BeautifulSoup –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞!")
        
        self.session = requests.Session()
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FlareSolverr –ø–∞—Ä—Å–µ—Ä–∞...")
        await self._test_flaresolverr()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –ø–∞—Ä—Å–µ—Ä–∞"""
        if self.session:
            self.session.close()
        self.session = None
    
    async def _test_flaresolverr(self):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ FlareSolverr"""
        try:
            test_payload = {
                "cmd": "request.get",
                "url": "https://httpbin.org/ip",
                "maxTimeout": 30000
            }
            
            response = await asyncio.to_thread(
                self.session.post, 
                self.flaresolverr_url, 
                json=test_payload, 
                timeout=35
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('status') == 'ok':
                    print("‚úÖ FlareSolverr –ø–æ–¥–∫–ª—é—á–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                    return True
                else:
                    print(f"‚ö†Ô∏è FlareSolverr –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {result.get('message', 'Unknown')}")
                    return True
            else:
                print(f"‚ö†Ô∏è FlareSolverr —Å—Ç–∞—Ç—É—Å: {response.status_code}")
                return True
                
        except requests.exceptions.ConnectionError:
            print("‚ùå FlareSolverr –Ω–µ –∑–∞–ø—É—â–µ–Ω!")
            print("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ: docker run -p 8191:8191 ghcr.io/flaresolverr/flaresolverr:latest")
            raise ConnectionError("FlareSolverr –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è FlareSolverr: {e}")
            return True
    
    async def _get_page_via_flaresolverr(self, url: str, use_cache: bool = True) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ FlareSolverr"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if use_cache and url in self.html_cache:
            cached_data = self.html_cache[url]
            if time.time() - cached_data['timestamp'] < self.cache_timeout:
                print("üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
                return cached_data['html']
        
        print(f"üåê –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ FlareSolverr: {url}")
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º payload –¥–ª—è FlareSolverr
            payload = {
                "cmd": "request.get",
                "url": url,
                "maxTimeout": 60000,
                "userAgent": self.mobile_headers['User-Agent']
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–∫—Å–∏ –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω
            proxy_url = self._build_proxy_url()
            if proxy_url:
                payload["proxy"] = {"url": proxy_url}
                print(f"üåê –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–∫—Å–∏: {self.proxy_config['host']}:{self.proxy_config['port']}")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
            response = await asyncio.to_thread(
                self.session.post,
                self.flaresolverr_url,
                json=payload,
                timeout=70
            )
            
            if response.status_code == 200:
                result = response.json()
                
                if result.get('status') == 'ok':
                    solution = result.get('solution', {})
                    status_code = solution.get('status', 0)
                    html = solution.get('response', '')
                    
                    print(f"‚úÖ FlareSolverr —Å—Ç–∞—Ç—É—Å: {status_code}")
                    print(f"üìè –†–∞–∑–º–µ—Ä HTML: {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
                    
                    if status_code == 200 and html:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ Ozon —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                        if "ozon" in html.lower() and ("product" in html.lower() or "—Ç–æ–≤–∞—Ä" in html.lower()):
                            print("‚úÖ Ozon —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–ª—É—á–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            debug_file = f"ozon_debug_{timestamp}.html"
                            try:
                                with open(debug_file, "w", encoding="utf-8") as f:
                                    f.write(html)
                                print(f"üíæ HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏: {debug_file}")
                            except Exception as e:
                                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å HTML: {e}")
                            
                            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                            if use_cache:
                                self.html_cache[url] = {
                                    'html': html,
                                    'timestamp': time.time()
                                }
                            
                            return html
                        else:
                            print("‚ö†Ô∏è HTML –ø–æ–ª—É—á–µ–Ω, –Ω–æ –Ω–µ –ø–æ—Ö–æ–∂ –Ω–∞ Ozon —Å—Ç—Ä–∞–Ω–∏—Ü—É")
                            return None
                    else:
                        print(f"‚ùå HTTP –æ—à–∏–±–∫–∞: {status_code}")
                        return None
                else:
                    print(f"‚ùå FlareSolverr –æ—à–∏–±–∫–∞: {result.get('message', 'Unknown error')}")
                    return None
            else:
                print(f"‚ùå FlareSolverr —Å–µ—Ä–≤–µ—Ä –æ—à–∏–±–∫–∞: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
            return None
    
    def _extract_product_id(self, product_url: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç ID —Ç–æ–≤–∞—Ä–∞ –∏–∑ URL"""
        if '/product/' in product_url:
            parts = product_url.split('/product/')
            if len(parts) > 1:
                product_part = parts[1].rstrip('/')
                # –ò—â–µ–º —á–∏—Å–ª–æ–≤–æ–π ID –≤ –∫–æ–Ω—Ü–µ URL
                match = re.search(r'(\d+)/?$', product_part)
                if match:
                    return match.group(1)
        return None
    
    def _extract_price_from_html(self, html: str) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω—É –∏–∑ HTML"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # –ò—â–µ–º —Ü–µ–Ω—É –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–∞—Ö Ozon
            price_selectors = [
                # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã —Ü–µ–Ω
                'span[data-testid="price"]',
                '.price',
                '.product-price',
                '[class*="price"]',
                '[class*="kz1"]',  # Ozon —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∫–ª–∞—Å—Å—ã
                '[class*="kz2"]',
                '[class*="kz6"]',
                '[class*="kz7"]',
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                'span[class*="tsBodyNumeric"]',
                'span[class*="tsHeadlineNumeric"]',
                '[data-widget="webPrice"]',
                '[data-widget="price"]',
                # –ù–æ–≤—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è Ozon
                'span[class*="kz8"]',
                'span[class*="kz9"]',
                'span[class*="kz10"]',
                'span[class*="kz11"]',
                'span[class*="kz12"]',
                'span[class*="kz13"]',
                'span[class*="kz14"]',
                'span[class*="kz15"]',
                'span[class*="kz16"]',
                'span[class*="kz17"]',
                'span[class*="kz18"]',
                'span[class*="kz19"]',
                'span[class*="kz20"]',
                # –°–µ–ª–µ–∫—Ç–æ—Ä—ã —Å —á–∏—Å–ª–∞–º–∏
                'span[class*="numeric"]',
                'span[class*="amount"]',
                'span[class*="cost"]',
                'span[class*="value"]',
                # –°–µ–ª–µ–∫—Ç–æ—Ä—ã —Å —Ä—É–±–ª—è–º–∏
                'span:contains("‚ÇΩ")',
                'span:contains("—Ä—É–±")',
                'span:contains("—Ä—É–±–ª–µ–π")',
                # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                'span[class*="ts"]',
                'div[class*="price"]',
                'div[class*="cost"]',
                'div[class*="amount"]'
            ]
            
            current_price = None
            original_price = None
            
            # –ò—â–µ–º —Ç–µ–∫—É—â—É—é —Ü–µ–Ω—É
            for selector in price_selectors:
                try:
                    elements = soup.select(selector)
                    for elem in elements:
                        text = elem.get_text().strip()
                        if text and re.search(r'\d', text):
                            # –û—á–∏—â–∞–µ–º —Ü–µ–Ω—É –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                            price_clean = re.sub(r'[^\d]', '', text)
                            if price_clean and len(price_clean) >= 3:  # –ú–∏–Ω–∏–º—É–º 3 —Ü–∏—Ñ—Ä—ã
                                try:
                                    price_num = int(price_clean)
                                    # –ë–µ—Ä–µ–º —Å–∞–º—É—é –º–∞–ª–µ–Ω—å–∫—É—é —Ü–µ–Ω—É –∫–∞–∫ —Ç–µ–∫—É—â—É—é
                                    if not current_price or price_num < current_price:
                                        current_price = price_num
                                        print(f"üîç –ù–∞–π–¥–µ–Ω–∞ —Ü–µ–Ω–∞ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä '{selector}': {price_num}")
                                except ValueError:
                                    continue
                except Exception as e:
                    continue
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
            if not current_price:
                print("üîç –ü–æ–∏—Å–∫ —Ü–µ–Ω—ã —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è...")
                # –ò—â–µ–º —Ü–µ–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ HTML
                price_patterns = [
                    r'(\d{3,})\s*‚ÇΩ',  # 1234 ‚ÇΩ
                    r'(\d{3,})\s*—Ä—É–±',  # 1234 —Ä—É–±
                    r'(\d{3,})\s*—Ä—É–±–ª–µ–π',  # 1234 —Ä—É–±–ª–µ–π
                    r'‚ÇΩ\s*(\d{3,})',  # ‚ÇΩ 1234
                    r'—Ä—É–±\s*(\d{3,})',  # —Ä—É–± 1234
                    r'"price":\s*(\d{3,})',  # "price": 1234
                    r'"amount":\s*(\d{3,})',  # "amount": 1234
                    r'"value":\s*(\d{3,})',  # "value": 1234
                    r'"cost":\s*(\d{3,})',  # "cost": 1234
                ]
                
                for pattern in price_patterns:
                    matches = re.findall(pattern, html, re.IGNORECASE)
                    for match in matches:
                        try:
                            price_num = int(match)
                            if 100 <= price_num <= 1000000:  # –†–∞–∑—É–º–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–Ω
                                if not current_price or price_num < current_price:
                                    current_price = price_num
                                    print(f"üîç –ù–∞–π–¥–µ–Ω–∞ —Ü–µ–Ω–∞ —á–µ—Ä–µ–∑ regex '{pattern}': {price_num}")
                        except ValueError:
                            continue
            
            # –ò—â–µ–º —Å—Ç–∞—Ä—É—é —Ü–µ–Ω—É (–∑–∞—á–µ—Ä–∫–Ω—É—Ç—É—é)
            old_price_selectors = [
                'span[class*="old"]',
                'span[class*="cross"]',
                'del',
                's',
                '[class*="strikethrough"]',
                '[class*="original"]',
                '[class*="base"]'
            ]
            
            for selector in old_price_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    text = elem.get_text().strip()
                    if text and re.search(r'\d', text):
                        price_clean = re.sub(r'[^\d]', '', text)
                        if price_clean and len(price_clean) >= 3:
                            try:
                                price_num = int(price_clean)
                                # –ë–µ—Ä–µ–º —Å–∞–º—É—é –±–æ–ª—å—à—É—é —Ü–µ–Ω—É –∫–∞–∫ —Å—Ç–∞—Ä—É—é
                                if not original_price or price_num > original_price:
                                    original_price = price_num
                            except ValueError:
                                continue
            
            if current_price:
                result = {
                    'current_price': current_price,
                    'currency': 'RUB'
                }
                
                if original_price and original_price > current_price:
                    result['original_price'] = original_price
                    discount = ((original_price - current_price) / original_price) * 100
                    result['discount_percent'] = round(discount, 2)
                
                print(f"üí∞ –ù–∞–π–¥–µ–Ω–∞ —Ü–µ–Ω–∞: {current_price} RUB")
                if original_price:
                    print(f"üí∏ –°—Ç–∞—Ä–∞—è —Ü–µ–Ω–∞: {original_price} RUB (—Å–∫–∏–¥–∫–∞ {result['discount_percent']}%)")
                
                return result
            
            print("‚ö†Ô∏è –¶–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ HTML")
            return None
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ü–µ–Ω—ã: {e}")
            return None
    
    def _extract_reviews_from_html(self, html: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ç–∑—ã–≤—ã –∏–∑ HTML"""
        reviews = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # –ò—â–µ–º JSON –¥–∞–Ω–Ω—ã–µ –≤ script —Ç–µ–≥–∞—Ö
            scripts = soup.find_all('script')
            
            for script in scripts:
                if script.string and ('review' in script.string.lower() or '–æ—Ç–∑—ã–≤' in script.string.lower()):
                    try:
                        script_text = script.string
                        
                        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å –æ—Ç–∑—ã–≤–∞–º–∏ –≤ JSON
                        review_patterns = [
                            r'"reviews"\s*:\s*(\[.*?\])',
                            r'"feedback"\s*:\s*(\[.*?\])',
                            r'"comments"\s*:\s*(\[.*?\])',
                            r'"items"\s*:\s*(\[.*?\])',
                            r'"data"\s*:\s*(\[.*?\])'
                        ]
                        
                        for pattern in review_patterns:
                            matches = re.findall(pattern, script_text, re.DOTALL)
                            for match in matches:
                                try:
                                    reviews_data = json.loads(match)
                                    if isinstance(reviews_data, list):
                                        for review_data in reviews_data:
                                            parsed_review = self._parse_single_review_from_data(review_data)
                                            if parsed_review:
                                                reviews.append(parsed_review)
                                except json.JSONDecodeError:
                                    continue
                    except Exception:
                        continue
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ JSON, –∏—â–µ–º –≤ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
            if not reviews:
                print("üîç –ü–æ–∏—Å–∫ –æ—Ç–∑—ã–≤–æ–≤ –≤ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä–µ...")
                review_selectors = [
                    '[class*="review"]',
                    '[class*="feedback"]',
                    '[class*="comment"]',
                    '[class*="–æ—Ç–∑—ã–≤"]',
                    '[class*="–æ—Ç–∑—ã–≤—ã"]',
                    '[class*="reviews"]',
                    '[class*="rating"]',
                    '[class*="–æ—Ü–µ–Ω–∫–∞"]',
                    '[class*="–æ—Ü–µ–Ω–∫–∏"]',
                    '[data-testid*="review"]',
                    '[data-testid*="feedback"]',
                    '[data-testid*="comment"]',
                    '[data-widget*="review"]',
                    '[data-widget*="feedback"]',
                    '[data-widget*="comment"]',
                    'div[class*="item"]',
                    'div[class*="card"]',
                    'div[class*="block"]'
                ]
                
                for selector in review_selectors:
                    try:
                        review_elements = soup.select(selector)
                        for elem in review_elements[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                            review = self._parse_review_from_element(elem)
                            if review and review['text'] and len(review['text']) > 20:
                                reviews.append(review)
                                print(f"üîç –ù–∞–π–¥–µ–Ω –æ—Ç–∑—ã–≤ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä '{selector}': {review['author']}")
                    except Exception as e:
                        continue
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤ widgetStates
            if not reviews:
                reviews.extend(self._extract_reviews_from_widget_states(html))
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
            if not reviews:
                print("üîç –ü–æ–∏—Å–∫ –æ—Ç–∑—ã–≤–æ–≤ —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è...")
                review_patterns = [
                    r'"text":\s*"([^"]{20,})"',  # "text": "–æ—Ç–∑—ã–≤"
                    r'"comment":\s*"([^"]{20,})"',  # "comment": "–æ—Ç–∑—ã–≤"
                    r'"content":\s*"([^"]{20,})"',  # "content": "–æ—Ç–∑—ã–≤"
                    r'"review":\s*"([^"]{20,})"',  # "review": "–æ—Ç–∑—ã–≤"
                    r'"feedback":\s*"([^"]{20,})"',  # "feedback": "–æ—Ç–∑—ã–≤"
                    r'"message":\s*"([^"]{20,})"',  # "message": "–æ—Ç–∑—ã–≤"
                ]
                
                for pattern in review_patterns:
                    matches = re.findall(pattern, html, re.IGNORECASE | re.DOTALL)
                    for i, match in enumerate(matches[:10]):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                        if len(match) > 20:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–∑—ã–≤–∞
                            review = {
                                'id': f'regex_{i}',
                                'author': '–ê–Ω–æ–Ω–∏–º',
                                'text': match,
                                'rating': 0,
                                'date': '',
                                'pros': '',
                                'cons': '',
                                'useful_count': 0,
                                'is_anonymous': True,
                                'status': 'regex_found'
                            }
                            reviews.append(review)
                            print(f"üîç –ù–∞–π–¥–µ–Ω –æ—Ç–∑—ã–≤ —á–µ—Ä–µ–∑ regex '{pattern}': {match[:50]}...")
            
            print(f"üìù –ù–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤: {len(reviews)}")
            return reviews
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Ç–∑—ã–≤–æ–≤: {e}")
            return []
    
    def _extract_reviews_from_widget_states(self, html: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ç–∑—ã–≤—ã –∏–∑ widgetStates –≤ HTML"""
        reviews = []
        
        try:
            # –ò—â–µ–º widgetStates –≤ HTML
            widget_states_match = re.search(r'window\.__NUXT__\s*=\s*({.*?});', html, re.DOTALL)
            if widget_states_match:
                nuxt_data = json.loads(widget_states_match.group(1))
                
                # –ò—â–µ–º –æ—Ç–∑—ã–≤—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–∞—Ö
                if 'data' in nuxt_data:
                    data = nuxt_data['data']
                    if isinstance(data, list):
                        for item in data:
                            if isinstance(item, dict) and 'reviews' in item:
                                reviews_data = item['reviews']
                                if isinstance(reviews_data, list):
                                    for review_data in reviews_data:
                                        parsed_review = self._parse_single_review_from_data(review_data)
                                        if parsed_review:
                                            reviews.append(parsed_review)
        
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ widgetStates: {e}")
        
        return reviews
    
    def _parse_single_review_from_data(self, review_data: Dict) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞ –∏–∑ JSON –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä
            author = '–ê–Ω–æ–Ω–∏–º'
            if isinstance(review_data.get('author'), dict):
                author = review_data['author'].get('firstName', '–ê–Ω–æ–Ω–∏–º')
            elif isinstance(review_data.get('author'), str):
                author = review_data['author']
            
            text = review_data.get('text', '') or review_data.get('comment', '') or review_data.get('content', '')
            if isinstance(text, dict):
                text = text.get('comment', '') or text.get('text', '')
            
            rating = review_data.get('rating', 0) or review_data.get('score', 0) or review_data.get('stars', 0)
            if isinstance(rating, dict):
                rating = rating.get('value', 0)
            
            date = review_data.get('date', '') or review_data.get('created_at', '') or review_data.get('publishedAt', '')
            
            pros = review_data.get('pros', '') or review_data.get('advantages', '')
            cons = review_data.get('cons', '') or review_data.get('disadvantages', '')
            
            useful_count = review_data.get('useful', 0) or review_data.get('likes', 0) or review_data.get('helpful', 0)
            if isinstance(useful_count, dict):
                useful_count = useful_count.get('count', 0)
            
            return {
                'id': review_data.get('id', '') or review_data.get('uuid', ''),
                'author': author,
                'text': str(text),
                'rating': int(rating) if rating else 0,
                'date': str(date),
                'pros': str(pros),
                'cons': str(cons),
                'useful_count': int(useful_count) if useful_count else 0,
                'is_anonymous': review_data.get('isAnonymous', False) or review_data.get('anonymous', False),
                'status': review_data.get('status', '') or review_data.get('state', '')
            }
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–∑—ã–≤–∞ –∏–∑ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return None
    
    def _parse_review_from_element(self, element) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–∞ –∏–∑ HTML —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            # –ò—â–µ–º –∞–≤—Ç–æ—Ä–∞
            author_elem = element.select_one('[class*="author"], [class*="user"], [class*="name"], [class*="–∞–≤—Ç–æ—Ä"]')
            author = author_elem.get_text().strip() if author_elem else '–ê–Ω–æ–Ω–∏–º'
            
            # –ò—â–µ–º —Ç–µ–∫—Å—Ç
            text_elem = element.select_one('[class*="text"], [class*="content"], [class*="comment"], [class*="–æ—Ç–∑—ã–≤"]')
            text = text_elem.get_text().strip() if text_elem else ''
            
            # –ò—â–µ–º —Ä–µ–π—Ç–∏–Ω–≥
            rating_elem = element.select_one('[class*="rating"], [class*="star"], [class*="score"], [class*="—Ä–µ–π—Ç–∏–Ω–≥"]')
            rating = 0
            if rating_elem:
                rating_text = rating_elem.get_text().strip()
                rating_match = re.search(r'(\d+)', rating_text)
                if rating_match:
                    rating = int(rating_match.group(1))
            
            # –ò—â–µ–º –¥–∞—Ç—É
            date_elem = element.select_one('[class*="date"], [class*="time"], [class*="–¥–∞—Ç–∞"]')
            date = date_elem.get_text().strip() if date_elem else ''
            
            if text and len(text) > 10:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞
                return {
                    'id': '',
                    'author': author,
                    'text': text,
                    'rating': rating,
                    'date': date,
                    'pros': '',
                    'cons': '',
                    'useful_count': 0,
                    'is_anonymous': False,
                    'status': ''
                }
            
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–∑—ã–≤–∞ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
            return None
    
    async def get_product_price(self, product_url: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ü–µ–Ω—ã —Ç–æ–≤–∞—Ä–∞"""
        print(f"üí∞ –ü–∞—Ä—Å–∏–Ω–≥ —Ü–µ–Ω—ã: {product_url}")
        
        html = await self._get_page_via_flaresolverr(product_url)
        if not html:
            print("‚ùå HTML —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –ø–æ–ª—É—á–µ–Ω–∞")
            return None
        
        price_info = self._extract_price_from_html(html)
        if price_info:
            price_info['product_id'] = self._extract_product_id(product_url)
        
        return price_info
    
    async def get_product_reviews(self, product_url: str, max_pages: int = 3) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ —Ç–æ–≤–∞—Ä–∞"""
        print(f"üìù –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤: {product_url}")
        
        all_reviews = []
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç–æ–≤–∞—Ä–∞
        html = await self._get_page_via_flaresolverr(product_url)
        if not html:
            print("‚ùå HTML —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –ø–æ–ª—É—á–µ–Ω–∞")
            return []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–∑—ã–≤—ã —Å –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        reviews = self._extract_reviews_from_html(html)
        all_reviews.extend(reviews)
        
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—Ç–∑—ã–≤–æ–≤
        product_id = self._extract_product_id(product_url)
        if product_id and max_pages > 1:
            for page in range(2, max_pages + 1):
                try:
                    # URL –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—Ç–∑—ã–≤–æ–≤
                    reviews_url = f"https://www.ozon.ru/product/{product_id}/reviews/?page={page}"
                    
                    print(f"üìÑ –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—Ç–∑—ã–≤–æ–≤ {page}")
                    page_html = await self._get_page_via_flaresolverr(reviews_url)
                    
                    if page_html:
                        page_reviews = self._extract_reviews_from_html(page_html)
                        if page_reviews:
                            all_reviews.extend(page_reviews)
                            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(page_reviews)} –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}")
                        else:
                            print(f"‚úÖ –û—Ç–∑—ã–≤—ã –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}")
                            break
                    else:
                        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}")
                        break
                    
                    # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    await asyncio.sleep(random.uniform(2, 4))
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")
                    break
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID
        unique_reviews = []
        seen_ids = set()
        for review in all_reviews:
            review_id = review.get('id', '')
            if review_id and review_id in seen_ids:
                continue
            if review_id:
                seen_ids.add(review_id)
            unique_reviews.append(review)
        
        print(f"üéâ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤: {len(unique_reviews)}")
        return unique_reviews
    
    async def get_product_full_info(self, product_url: str) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–≤–∞—Ä–µ"""
        print(f"üîç –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–≤–∞—Ä–∞: {product_url}")
        
        result = {
            'url': product_url,
            'product_id': self._extract_product_id(product_url),
            'price_info': None,
            'reviews': [],
            'reviews_count': 0,
            'average_rating': 0,
            'parsed_at': datetime.now().isoformat()
        }
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –ø–æ–ª—É—á–∞–µ–º —Ü–µ–Ω—É –∏ –æ—Ç–∑—ã–≤—ã
        price_task = self.get_product_price(product_url)
        reviews_task = self.get_product_reviews(product_url)
        
        price_info, reviews = await asyncio.gather(price_task, reviews_task, return_exceptions=True)
        
        if not isinstance(price_info, Exception) and price_info:
            result['price_info'] = price_info
        
        if not isinstance(reviews, Exception) and reviews:
            result['reviews'] = reviews
            result['reviews_count'] = len(reviews)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥
            ratings = [r['rating'] for r in reviews if r.get('rating') and r['rating'] > 0]
            if ratings:
                result['average_rating'] = round(sum(ratings) / len(ratings), 2)
        
        return result
    
    async def parse_products_bulk(self, product_urls: List[str]) -> List[Dict]:
        """–ú–∞—Å—Å–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤"""
        print(f"üöÄ –ú–∞—Å—Å–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ {len(product_urls)} —Ç–æ–≤–∞—Ä–æ–≤")
        
        results = []
        
        for i, url in enumerate(product_urls, 1):
            print(f"\nüì¶ –¢–æ–≤–∞—Ä {i}/{len(product_urls)}")
            
            try:
                result = await self.get_product_full_info(url)
                results.append(result)
                
                print(f"‚úÖ –¢–æ–≤–∞—Ä {i} –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ")
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–æ–≤–∞—Ä–∞–º–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                if i < len(product_urls):
                    delay = random.uniform(5, 10)
                    print(f"‚è≥ –ü–∞—É–∑–∞ {delay:.1f}—Å...")
                    await asyncio.sleep(delay)
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–≤–∞—Ä–∞ {i}: {e}")
                results.append({
                    'url': url,
                    'error': str(e),
                    'parsed_at': datetime.now().isoformat()
                })
        
        print(f"\nüéâ –ú–∞—Å—Å–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return results


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def parse_ozon_flaresolverr_price(product_url: str, proxy_config: Optional[Dict] = None) -> Optional[Dict]:
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ü–µ–Ω—ã —á–µ—Ä–µ–∑ FlareSolverr"""
    async with OzonFlareSolverrParser(proxy_config) as parser:
        return await parser.get_product_price(product_url)

async def parse_ozon_flaresolverr_reviews(product_url: str, proxy_config: Optional[Dict] = None) -> List[Dict]:
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–∑—ã–≤–æ–≤ —á–µ—Ä–µ–∑ FlareSolverr"""
    async with OzonFlareSolverrParser(proxy_config) as parser:
        return await parser.get_product_reviews(product_url)

async def parse_ozon_flaresolverr_full(product_url: str, proxy_config: Optional[Dict] = None) -> Dict:
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ FlareSolverr"""
    async with OzonFlareSolverrParser(proxy_config) as parser:
        return await parser.get_product_full_info(product_url)


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã FlareSolverr –ø–∞—Ä—Å–µ—Ä–∞"""
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–∫—Å–∏
    proxy_config = {
        'scheme': 'http',
        'host': os.getenv('OZON_PROXY_HOST', 'p15184.ltespace.net'),
        'port': int(os.getenv('OZON_PROXY_PORT', '15184')),
        'username': os.getenv('OZON_PROXY_USERNAME', 'uek7t66y'),
        'password': os.getenv('OZON_PROXY_PASSWORD', 'zbygddap')
    }
    
    # –ü—Ä–∏–º–µ—Ä URL —Ç–æ–≤–∞—Ä–∞
    product_url = "https://www.ozon.ru/product/termozashchitnyy-sprey-dlya-volos-uvlazhnyayushchiy-nesmyvaemyy-uhod-dlya-legkogo-2128381166/?__rr=3&at=EqtkV5nBRhyWXGM9iY1OEWVhDKJLXvsrZVAMkFZK70J2"
    
    print("üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã Ozon FlareSolverr Parser")
    
    async with OzonFlareSolverrParser(proxy_config) as parser:
        # –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–≤–∞—Ä–∞
        result = await parser.get_product_full_info(product_url)
        
        print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢ –ü–ê–†–°–ò–ù–ì–ê:")
        print(f"–¢–æ–≤–∞—Ä: {result['url']}")
        print(f"ID: {result['product_id']}")
        
        if result['price_info']:
            price = result['price_info']
            print(f"üí∞ –¶–µ–Ω–∞: {price['current_price']} {price['currency']}")
            if price.get('original_price'):
                print(f"üí∏ –°—Ç–∞—Ä–∞—è —Ü–µ–Ω–∞: {price['original_price']} {price['currency']}")
                print(f"üéØ –°–∫–∏–¥–∫–∞: {price['discount_percent']}%")
        
        print(f"üìù –û—Ç–∑—ã–≤–æ–≤: {result['reviews_count']}")
        if result['average_rating']:
            print(f"‚≠ê –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {result['average_rating']}/5")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –æ—Ç–∑—ã–≤–∞
        for i, review in enumerate(result['reviews'][:3], 1):
            print(f"\nüìÑ –û—Ç–∑—ã–≤ {i}:")
            print(f"   –ê–≤—Ç–æ—Ä: {review['author']}")
            print(f"   –†–µ–π—Ç–∏–Ω–≥: {review['rating']}/5")
            print(f"   –¢–µ–∫—Å—Ç: {review['text'][:100]}...")


if __name__ == "__main__":
    asyncio.run(main())
