#!/usr/bin/env python3
"""
Ozon HTML Parser - –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤–º–µ—Å—Ç–æ API
–û–±—Ö–æ–¥–∏—Ç 403 –æ—à–∏–±–∫–∏, –ø–∞—Ä—Å—è –¥–∞–Ω–Ω—ã–µ –ø—Ä—è–º–æ –∏–∑ HTML
"""

import asyncio
import json
import os
import random
import re
from datetime import datetime
from typing import Dict, List, Optional
from urllib.parse import quote, urljoin, urlparse
import uuid

try:
    from curl_cffi import requests as curl_requests
    HAS_CURL_CFFI = True
except ImportError:
    HAS_CURL_CFFI = False

try:
    import httpx
    HAS_HTTPX = True
except ImportError:
    HAS_HTTPX = False

from bs4 import BeautifulSoup

class OzonHTMLParser:
    """–ü–∞—Ä—Å–µ—Ä Ozon —á–µ—Ä–µ–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    
    def __init__(self, proxy_config: Optional[Dict] = None):
        self.proxy_config = proxy_config
        self.session = None
        self.cookies = {}
        self.device_id = str(uuid.uuid4())
        self.session_id = str(uuid.uuid4())
        
        # –ú–æ–±–∏–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        self.mobile_headers = {
            'User-Agent': 'Mozilla/5.0 (Linux; Android 12; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.210 Mobile Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-User': '?1',
            'Sec-Fetch-Dest': 'document',
            'Cache-Control': 'max-age=0',
        }
        
    def _build_proxy_url(self) -> Optional[str]:
        """–°–æ–±–∏—Ä–∞–µ—Ç URL –ø—Ä–æ–∫—Å–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞"""
        if not self.proxy_config:
            return None
        
        config = self.proxy_config
        auth = ""
        if config.get('username') and config.get('password'):
            auth = f"{config['username']}:{config['password']}@"
        
        return f"{config.get('scheme', 'http')}://{auth}{config['host']}:{config['port']}"
    
    async def __aenter__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
        if not HAS_CURL_CFFI:
            raise ImportError("curl_cffi –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞!")
        
        proxy_url = self._build_proxy_url()
        proxies = None
        if proxy_url:
            proxies = {
                'http': proxy_url,
                'https': proxy_url
            }
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é
        impersonate_id = os.getenv("OZON_CURL_IMPERSONATE", "chrome120")
        self.session = curl_requests.Session(
            impersonate=impersonate_id,
            headers=self.mobile_headers,
            proxies=proxies,
            timeout=30,
            verify=True
        )
        
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è HTML –ø–∞—Ä—Å–µ—Ä–∞...")
        await self._warmup_session()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –ø–∞—Ä—Å–µ—Ä–∞"""
        self.session = None
    
    async def _warmup_session(self):
        """–ü—Ä–æ–≥—Ä–µ–≤ —Å–µ—Å—Å–∏–∏"""
        print("üî• –ü—Ä–æ–≥—Ä–µ–≤ —Å–µ—Å—Å–∏–∏...")
        
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–≥—Ä–µ–≤ - —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
            await self._safe_request('GET', 'https://www.ozon.ru/')
            await asyncio.sleep(random.uniform(2, 4))
            print("‚úÖ –°–µ—Å—Å–∏—è –ø—Ä–æ–≥—Ä–µ—Ç–∞ —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–æ–≥—Ä–µ–≤–µ: {e}")
    
    async def _safe_request(self, method: str, url: str, **kwargs) -> Optional[object]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å —Ñ–æ–ª–±—ç–∫–æ–º –Ω–∞ httpx"""
        try:
            if method.upper() == 'GET':
                response = await asyncio.to_thread(self.session.get, url, **kwargs)
            elif method.upper() == 'POST':
                response = await asyncio.to_thread(self.session.post, url, **kwargs)
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –º–µ—Ç–æ–¥: {method}")

            if hasattr(response, 'cookies'):
                self.cookies.update(response.cookies)
            return response
        except Exception as e:
            print(f"‚ùå curl_cffi –æ—à–∏–±–∫–∞ {method} {url}: {e}")
            # –§–æ–ª–±—ç–∫ –Ω–∞ httpx
            if HAS_HTTPX:
                try:
                    proxy_url = self._build_proxy_url()
                    httpx_proxies = None
                    if proxy_url:
                        httpx_proxies = {"http://": proxy_url, "https://": proxy_url}
                    async with httpx.AsyncClient(timeout=30.0, proxies=httpx_proxies) as client:
                        resp = await client.request(
                            method=method.upper(),
                            url=url,
                            headers=kwargs.get('headers') or self.mobile_headers,
                            params=kwargs.get('params'),
                            json=kwargs.get('json'),
                            data=kwargs.get('data')
                        )
                        # –ò–º–∏—Ç–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç Response
                        class _Shim:
                            def __init__(self, r):
                                self.status_code = r.status_code
                                self.text = r.text
                                self.headers = r.headers
                                self._json = None
                                try:
                                    self._json = r.json()
                                except Exception:
                                    self._json = None
                                self.cookies = r.cookies
                            def json(self):
                                if self._json is not None:
                                    return self._json
                                raise ValueError("No JSON")
                        return _Shim(resp)
                except Exception as e2:
                    print(f"‚ùå httpx —Ñ–æ–ª–±—ç–∫ —Ç–æ–∂–µ —É–ø–∞–ª: {e2}")
                    return None
            return None
    
    def _extract_product_id(self, product_url: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç ID —Ç–æ–≤–∞—Ä–∞ –∏–∑ URL"""
        if '/product/' in product_url:
            parts = product_url.split('/product/')
            if len(parts) > 1:
                product_part = parts[1].rstrip('/')
                match = re.search(r'(\d+)/?$', product_part)
                if match:
                    return match.group(1)
        return None
    
    async def get_product_page(self, product_url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç–æ–≤–∞—Ä–∞"""
        print(f"üìÑ –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–æ–≤–∞—Ä–∞: {product_url}")
        
        # –û—á–∏—â–∞–µ–º URL –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ª—É—á—à–µ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        clean_url = product_url.split('?')[0]
        
        headers = self.mobile_headers.copy()
        headers['Referer'] = 'https://www.ozon.ru/'
        
        response = await self._safe_request('GET', clean_url, headers=headers)
        
        if not response:
            print("‚ùå –ù–µ—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞")
            return None
        
        if response.status_code == 200:
            print(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–ª—É—á–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ ({len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return response.text
        elif response.status_code == 403:
            print("‚ùå 403 Forbidden - –∞–Ω—Ç–∏–±–æ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –∑–∞–ø—Ä–æ—Å")
            return None
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ {response.status_code}")
            return None
    
    def _extract_price_from_html(self, html: str) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω—É –∏–∑ HTML"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # –ò—â–µ–º —Ü–µ–Ω—É –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–∞—Ö
            price_selectors = [
                'span[data-testid="price"]',
                '.price',
                '.product-price',
                '[class*="price"]',
                'span[class*="kz1"]',  # Ozon —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∫–ª–∞—Å—Å—ã
                'span[class*="kz2"]',
                'span[class*="kz6"]',
                'span[class*="kz7"]',
            ]
            
            current_price = None
            original_price = None
            
            for selector in price_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    text = elem.get_text().strip()
                    if text and re.search(r'\d', text):
                        # –û—á–∏—â–∞–µ–º —Ü–µ–Ω—É
                        price_clean = re.sub(r'[^\d]', '', text)
                        if price_clean and len(price_clean) >= 3:  # –ú–∏–Ω–∏–º—É–º 3 —Ü–∏—Ñ—Ä—ã
                            price_num = int(price_clean)
                            if not current_price or price_num < current_price:
                                current_price = price_num
            
            # –ò—â–µ–º —Å—Ç–∞—Ä—É—é —Ü–µ–Ω—É (–∑–∞—á–µ—Ä–∫–Ω—É—Ç—É—é)
            old_price_selectors = [
                'span[class*="old"]',
                'span[class*="cross"]',
                'del',
                's',
                '[class*="strikethrough"]'
            ]
            
            for selector in old_price_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    text = elem.get_text().strip()
                    if text and re.search(r'\d', text):
                        price_clean = re.sub(r'[^\d]', '', text)
                        if price_clean and len(price_clean) >= 3:
                            price_num = int(price_clean)
                            if not original_price or price_num > original_price:
                                original_price = price_num
            
            if current_price:
                result = {
                    'current_price': current_price,
                    'currency': 'RUB'
                }
                
                if original_price and original_price > current_price:
                    result['original_price'] = original_price
                    discount = ((original_price - current_price) / original_price) * 100
                    result['discount_percent'] = round(discount, 2)
                
                return result
            
            return None
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ü–µ–Ω—ã: {e}")
            return None
    
    def _extract_reviews_from_html(self, html: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ç–∑—ã–≤—ã –∏–∑ HTML"""
        reviews = []
        
        try:
            # –ò—â–µ–º JSON –¥–∞–Ω–Ω—ã–µ –≤ script —Ç–µ–≥–∞—Ö
            soup = BeautifulSoup(html, 'html.parser')
            scripts = soup.find_all('script')
            
            for script in scripts:
                if script.string and 'reviews' in script.string.lower():
                    try:
                        # –ò—â–µ–º JSON —Å –æ—Ç–∑—ã–≤–∞–º–∏
                        script_text = script.string
                        
                        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "reviews": [...]
                        review_patterns = [
                            r'"reviews"\s*:\s*(\[.*?\])',
                            r'"feedback"\s*:\s*(\[.*?\])',
                            r'"comments"\s*:\s*(\[.*?\])'
                        ]
                        
                        for pattern in review_patterns:
                            matches = re.findall(pattern, script_text, re.DOTALL)
                            for match in matches:
                                try:
                                    reviews_data = json.loads(match)
                                    if isinstance(reviews_data, list):
                                        for review_data in reviews_data:
                                            parsed_review = self._parse_single_review_from_data(review_data)
                                            if parsed_review:
                                                reviews.append(parsed_review)
                                except:
                                    continue
                    except:
                        continue
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ JSON, –∏—â–µ–º –≤ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
            if not reviews:
                review_elements = soup.select('[class*="review"], [class*="feedback"], [class*="comment"]')
                for elem in review_elements[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    review = self._parse_review_from_element(elem)
                    if review:
                        reviews.append(review)
            
            return reviews
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Ç–∑—ã–≤–æ–≤: {e}")
            return []
    
    def _parse_single_review_from_data(self, review_data: Dict) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞ –∏–∑ JSON –¥–∞–Ω–Ω—ã—Ö"""
        try:
            return {
                'id': review_data.get('id', ''),
                'author': review_data.get('author', {}).get('name', '–ê–Ω–æ–Ω–∏–º') if isinstance(review_data.get('author'), dict) else str(review_data.get('author', '–ê–Ω–æ–Ω–∏–º')),
                'text': review_data.get('text', '') or review_data.get('comment', ''),
                'rating': review_data.get('rating', 0) or review_data.get('score', 0),
                'date': review_data.get('date', '') or review_data.get('created_at', ''),
                'pros': review_data.get('pros', ''),
                'cons': review_data.get('cons', ''),
                'useful_count': review_data.get('useful', 0) or review_data.get('likes', 0),
                'is_anonymous': review_data.get('anonymous', False),
                'status': review_data.get('status', '')
            }
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–∑—ã–≤–∞ –∏–∑ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return None
    
    def _parse_review_from_element(self, element) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–∞ –∏–∑ HTML —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            # –ò—â–µ–º –∞–≤—Ç–æ—Ä–∞
            author_elem = element.select_one('[class*="author"], [class*="user"], [class*="name"]')
            author = author_elem.get_text().strip() if author_elem else '–ê–Ω–æ–Ω–∏–º'
            
            # –ò—â–µ–º —Ç–µ–∫—Å—Ç
            text_elem = element.select_one('[class*="text"], [class*="content"], [class*="comment"]')
            text = text_elem.get_text().strip() if text_elem else ''
            
            # –ò—â–µ–º —Ä–µ–π—Ç–∏–Ω–≥
            rating_elem = element.select_one('[class*="rating"], [class*="star"], [class*="score"]')
            rating = 0
            if rating_elem:
                rating_text = rating_elem.get_text().strip()
                rating_match = re.search(r'(\d+)', rating_text)
                if rating_match:
                    rating = int(rating_match.group(1))
            
            if text:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç
                return {
                    'id': '',
                    'author': author,
                    'text': text,
                    'rating': rating,
                    'date': '',
                    'pros': '',
                    'cons': '',
                    'useful_count': 0,
                    'is_anonymous': False,
                    'status': ''
                }
            
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–∑—ã–≤–∞ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
            return None
    
    async def get_product_price(self, product_url: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ü–µ–Ω—ã —Ç–æ–≤–∞—Ä–∞"""
        print(f"üí∞ –ü–∞—Ä—Å–∏–Ω–≥ —Ü–µ–Ω—ã: {product_url}")
        
        html = await self.get_product_page(product_url)
        if not html:
            return None
        
        price_info = self._extract_price_from_html(html)
        if price_info:
            price_info['product_id'] = self._extract_product_id(product_url)
        
        return price_info
    
    async def get_product_reviews(self, product_url: str, max_pages: int = 3) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ —Ç–æ–≤–∞—Ä–∞"""
        print(f"üìù –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤: {product_url}")
        
        html = await self.get_product_page(product_url)
        if not html:
            return []
        
        reviews = self._extract_reviews_from_html(html)
        print(f"üéâ –ù–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤: {len(reviews)}")
        
        return reviews
    
    async def get_product_full_info(self, product_url: str) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–≤–∞—Ä–µ"""
        print(f"üîç –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–≤–∞—Ä–∞: {product_url}")
        
        result = {
            'url': product_url,
            'product_id': self._extract_product_id(product_url),
            'price_info': None,
            'reviews': [],
            'reviews_count': 0,
            'average_rating': 0,
            'parsed_at': datetime.now().isoformat()
        }
        
        # –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        html = await self.get_product_page(product_url)
        if not html:
            return result
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–Ω—É
        price_info = self._extract_price_from_html(html)
        if price_info:
            price_info['product_id'] = result['product_id']
            result['price_info'] = price_info
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–∑—ã–≤—ã
        reviews = self._extract_reviews_from_html(html)
        result['reviews'] = reviews
        result['reviews_count'] = len(reviews)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥
        if reviews:
            ratings = [r['rating'] for r in reviews if r.get('rating')]
            if ratings:
                result['average_rating'] = round(sum(ratings) / len(ratings), 2)
        
        return result


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def parse_ozon_html_price(product_url: str, proxy_config: Optional[Dict] = None) -> Optional[Dict]:
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ü–µ–Ω—ã —á–µ—Ä–µ–∑ HTML"""
    async with OzonHTMLParser(proxy_config) as parser:
        return await parser.get_product_price(product_url)

async def parse_ozon_html_reviews(product_url: str, proxy_config: Optional[Dict] = None) -> List[Dict]:
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–∑—ã–≤–æ–≤ —á–µ—Ä–µ–∑ HTML"""
    async with OzonHTMLParser(proxy_config) as parser:
        return await parser.get_product_reviews(product_url)

async def parse_ozon_html_full(product_url: str, proxy_config: Optional[Dict] = None) -> Dict:
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ HTML"""
    async with OzonHTMLParser(proxy_config) as parser:
        return await parser.get_product_full_info(product_url)


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã HTML –ø–∞—Ä—Å–µ—Ä–∞"""
    
    proxy_config = {
        'scheme': 'http',
        'host': os.getenv('OZON_PROXY_HOST', 'your.proxy.host'),
        'port': int(os.getenv('OZON_PROXY_PORT', '8080')),
        'username': os.getenv('OZON_PROXY_USERNAME', 'user'),
        'password': os.getenv('OZON_PROXY_PASSWORD', 'pass')
    }
    
    product_url = "https://www.ozon.ru/product/your-product-123456/"
    
    print("üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã Ozon HTML Parser")
    
    async with OzonHTMLParser(proxy_config) as parser:
        result = await parser.get_product_full_info(product_url)
        
        print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢ –ü–ê–†–°–ò–ù–ì–ê:")
        print(f"–¢–æ–≤–∞—Ä: {result['url']}")
        print(f"ID: {result['product_id']}")
        
        if result['price_info']:
            price = result['price_info']
            print(f"üí∞ –¶–µ–Ω–∞: {price['current_price']} {price['currency']}")
            if price.get('original_price'):
                print(f"üí∏ –°—Ç–∞—Ä–∞—è —Ü–µ–Ω–∞: {price['original_price']} {price['currency']}")
                print(f"üéØ –°–∫–∏–¥–∫–∞: {price['discount_percent']}%")
        
        print(f"üìù –û—Ç–∑—ã–≤–æ–≤: {result['reviews_count']}")
        if result['average_rating']:
            print(f"‚≠ê –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {result['average_rating']}/5")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –æ—Ç–∑—ã–≤–∞
        for i, review in enumerate(result['reviews'][:3], 1):
            print(f"\nüìÑ –û—Ç–∑—ã–≤ {i}:")
            print(f"   –ê–≤—Ç–æ—Ä: {review['author']}")
            print(f"   –†–µ–π—Ç–∏–Ω–≥: {review['rating']}/5")
            print(f"   –¢–µ–∫—Å—Ç: {review['text'][:100]}...")


if __name__ == "__main__":
    asyncio.run(main())
